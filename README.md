# SDN Security & Testing Framework

This project is an advanced framework for automated security, robustness, and performance testing of SDN (Software Defined Networking) controllers. The tool orchestrates complex test environments, executes a variety of workloads (benchmarks, DoS attacks, fault injection), and provides a graphical interface for result analysis.

Its main advantage is the **complete automation of the test environment lifecycle**, ensuring that each execution is isolated, clean, and reproducible.

## üèõÔ∏è Architecture and Workflow

The system operates in a distributed and automated manner, using Xen Orchestra for infrastructure management and gRPC for communication. The workflow of a typical test is as follows:

1.  **Start via GUI:** The user uses the graphical interface (`gui/backend/app.py`) to configure and start a new test.
2.  **Central Orchestration (`benchmark.py`):** The GUI request triggers the main orchestrator. This module is the brain of the framework and coordinates all subsequent steps.
3.  **Mininet VM Provisioning:** The orchestrator invokes the `xenorchestra.py` module, which communicates with the **Xen Orchestra** API to clone a Mininet VM from a pre-defined template. This ensures each test runs in a "clean" VM, eliminating interference from previous runs. The VM is destroyed at the end of the test.
4.  **SDN Controller Activation:** In parallel, the orchestrator connects via SSH to the Controller VM and uses **Docker** commands to start the specified SDN controller instance (ONOS, ODL, Ryu).
5.  **Communication via gRPC:** Once the VMs are ready, the orchestrator communicates with the `mininet_server.py` server (running on the Mininet VM) via a **gRPC API** (`mininet_control.proto`). This sends commands to build the network topology, generate traffic, and manipulate links.
6.  **Test Execution:** The orchestrator executes the test plan, which may include:
    *   Control plane performance benchmarks (`mimic_cbench.py`).
    *   Denial of Service (DoS) attacks like Xerxes and Slowloris (`attackload.py`).
    *   Fault injection into the REST API and malformed packets (`faultload.py`).
7.  **Result Collection and Analysis:** Results are saved in `.csv` files in the `Tool/output` directory. The GUI allows the user to visualize, process, and even merge results from multiple tests for comparative analysis.

## üöÄ Features

### 1. Orchestration and Automation
*   **Disposable Test Environments:** Integration with **Xen Orchestra** to automatically provision and destroy the network emulation VM (Mininet) for each test, ensuring total isolation.
*   **Controller Management via Docker:** Automates the lifecycle of SDN controllers (ONOS, ODL, Ryu) in Docker containers.
*   **Structured Communication:** Uses a gRPC API for robust and reliable control of the Mininet environment, instead of fragile SSH commands.

### 2. Security and Robustness Tests
*   **Denial of Service (DoS) Attacks:** Launches attacks like **Xerxes** (Application layer DoS) and **Slowloris** (Network layer DoS) to test controller resilience (`attackload.py`).
*   **Fault Injection:**
    *   **REST API Fuzzing:** Analyzes OpenAPI specifications (YAML) and injects invalid data into Northbound API endpoints to detect vulnerabilities.
    *   **Malformed Packets:** Sends corrupted packets to test data plane error handling.

### 3. Benchmarking and Performance Analysis
*   **Controller Performance Benchmark:** Uses `mimic_cbench.py` to measure control plane latency and throughput.
*   **Northbound API Tests:** Measures controller REST API performance (`northbound_api.py`).
*   **Proactive Provisioning:** Automates the installation of flow rules (ARP, ICMP, and L2 forwarding), ensuring a baseline network state for tests.

## üñ•Ô∏è Graphical Interface (GUI) - User Guide

The web interface, built with Flask (`gui/backend/app.py`), is the central point for visualizing and analyzing all results generated by the framework. It was designed as a powerful analytical dashboard, allowing not only the visualization of individual tests but also the statistical aggregation of multiple results.

Below is a detailed guide to its pages and internal API.

### Main Pages

These are the pages a user interacts with directly.

| Route (URL) | Template | Page Name | Main Functionality |
| :--- | :--- | :--- | :--- |
| `/` | `folders.html` | **Results Dashboard** | Home page. Lists all available result folders, including the virtual folder `Output_Realtime` for real-time monitoring. Indicates if a folder has already been processed or if it is the result of an aggregation (`merge`). |
| `/results/<folder>` | `results.html` | **Test Viewer** | The main analysis page. Loads and displays data from the selected result folder, rendering multiple charts and tables for each metric and test scenario. |
| `/merge` | `merge.html` | **Result Aggregation** | Presents an interface to select multiple result folders. The user can choose folders to combine and name the new aggregated dataset that will be generated. |

### Operational Logic and Internal API

The interface uses a set of API endpoints to load data dynamically and execute backend operations.

| Route (URL) | Method | API Description |
| :--- | :--- | :--- |
| `/api/results/<folder>` | `GET` | **Data Provider for Charts:** Endpoint called by the results page. Reads all `.csv` files in the specified folder, intelligently parses filenames to generate descriptive labels (e.g., `ONOS | MESH | API | THROUGHPUT`), and returns all data in JSON format. |
| `/api/merge` | `POST` | **Statistical Aggregation Engine:** Receives a list of folders and a new name. Creates a new result folder where each `.csv` is the **statistical average** (mean, min, max) of the corresponding files in the source folders. Essential for analyzing variability across multiple tests. |
| `/run-analysis` | `POST` | **Post-processing:** Executes the `make_results.py` script on a result folder to generate additional analyses or aggregations. |
| `/delete-folder` | `POST` | **Folder Management:** Allows deleting a result folder from the server (requires a security password to prevent accidental deletions). |

The filename parsing system (`parse_standard_filename` and `parse_ditg_filename` in `app.py`) is a key piece, enabling the GUI to automatically categorize and caption data according to:
*   Controller (ONOS, Ryu, ODL)
*   Topology (Mesh, 3-tier)
*   Mode (Proactive, Reactive, NN)
*   Metric (Latency, Throughput, RTT)
*   Test Scenario (Baseline, Malformed, REST, DoS)

## üìÇ Project Structure

*   `Tool/benchmark.py`: **The main orchestrator.** Coordinates the entire workflow.
*   `Tool/xenorchestra.py`: Integration module with Xen Orchestra for automatic Mininet VM management.
*   `Mininet/mininet_server.py`: gRPC server running on the Mininet VM executing received commands.
*   `Tool/mininet_control.proto`: The formal definition of the gRPC API between the orchestrator and the Mininet server.
*   `gui/backend/app.py`: The Flask application serving the graphical result analysis interface.
*   `Tool/faultload.py`: Fault injection engine (REST Fuzzing and Malformed Packets).
*   `Tool/attackload.py`: Module for launching Denial of Service (DoS) attacks.
*   `Tool/mimic_cbench.py`: Tool for controller performance benchmarking.
*   `Tool/run.py`: Helper script with functions to manage SDN controllers via SSH and Docker.

## üõ†Ô∏è Prerequisites

*   Python 3.x
*   API access to Xen Orchestra.
*   Pre-configured VMs (for the Controller and as a template for Mininet).
*   Docker installed on the Controller VM.

## üìÑ License

This project uses components under the CeCILL V2 and GNU GPLv3 licenses. See the `sourcesonoff` folder for more details.
